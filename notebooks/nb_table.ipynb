{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772f5577-75d2-4b12-af0f-e81fd4ffc163",
   "metadata": {},
   "source": [
    "# **LaTeX table with the BAO-fit results**\n",
    "\n",
    "This notebook shows how to create a LaTeX table with the BAO-fit results varying the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38b764-ac2c-49b9-affd-352119f0bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ed1ce-cc99-4dbb-83f7-91279d046c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c8ef6-d682-4355-960d-79401c2b63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "code_path = f\"{os.environ['PSCRATCH']}/BAOfit_wtheta\"\n",
    "save_path = f\"{os.environ['PSCRATCH']}/BAOfit_wtheta\"\n",
    "sys.path.append(code_path)\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = \"Times New Roman\"\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from utils_data import GetThetaLimits\n",
    "from utils_baofit import BAOFitInitializer\n",
    "from utils_cosmology import CosmologicalParameters\n",
    "\n",
    "old_template = \"_old\"\n",
    "# old_template = \"\"\n",
    "\n",
    "def rework_dataset_name(dataset):\n",
    "    if dataset == \"DESY6_dec_below-23.5\":\n",
    "        return r\"\\texttt{Deccut_noDESI}\"\n",
    "    elif dataset == \"DESY6_dec_above-23.5\":\n",
    "        return r\"\\texttt{Deccut_DESIonly}\"\n",
    "    elif dataset == \"DESY6_DR1tiles_noDESI\":\n",
    "        return r\"\\texttt{DR1tiles_noDESI}\"\n",
    "    elif dataset == \"DESY6_DR1tiles_DESIonly\":\n",
    "        return r\"\\texttt{DR1tiles_DESIonly}\"\n",
    "    elif dataset == \"DESY6\":\n",
    "        return \"Full sample\"\n",
    "    else:\n",
    "        return dataset\n",
    "\n",
    "dataset_list = [\"DESY6\", \"DESY6_DR1tiles_noDESI\", \"DESY6_DR1tiles_DESIonly\", \"DESY6_dec_below-23.5\", \"DESY6_dec_above-23.5\"]\n",
    "include_wiggles_list = [\"y\", \"n\"]\n",
    "# include_wiggles_list = [\"y\"]\n",
    "weight_type_list = [1, 0]\n",
    "# weight_type_list = [1]\n",
    "bins_removed_list = [[], [0], [1], [2], [3], [4], [5], [1, 2, 3, 4, 5], [0, 2, 3, 4, 5], \n",
    "                     [0, 1, 3, 4, 5], [0, 1, 2, 4, 5], [0, 1, 2, 3, 5], [0, 1, 2, 3, 4]]\n",
    "# bins_removed_list = [[]]\n",
    "\n",
    "nz_flag = \"fid\"\n",
    "dynamical_theta_limits = False\n",
    "\n",
    "fit_results = {}\n",
    "likelihoods = {}\n",
    "\n",
    "for dataset, include_wiggles, weight_type, bins_removed in product(dataset_list, include_wiggles_list, weight_type_list, bins_removed_list):\n",
    "    theta_min, theta_max = GetThetaLimits(dataset=dataset, nz_flag=nz_flag, dynamical_theta_limits=dynamical_theta_limits, code_path=code_path).get_theta_limits()\n",
    "\n",
    "    # 1. Arguments\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.include_wiggles = include_wiggles\n",
    "            self.dataset = dataset\n",
    "            self.weight_type = weight_type\n",
    "            self.mock_id = \"mean\" # it will not be used...\n",
    "            self.nz_flag = nz_flag\n",
    "            self.cov_type = \"cosmolike\"\n",
    "            self.cosmology_template = \"planck\" + old_template\n",
    "            self.cosmology_covariance = \"planck\"\n",
    "            self.delta_theta = 0.2\n",
    "            self.theta_min = theta_min\n",
    "            self.theta_max = theta_max\n",
    "            self.pow_broadband = [-2, -1, 0]\n",
    "            self.bins_removed = bins_removed\n",
    "            self.diag_only = \"n\"\n",
    "            self.remove_crosscov = \"n\"\n",
    "            self.alpha_min = 0.8\n",
    "            self.alpha_max = 1.2\n",
    "            self.code_path = code_path\n",
    "            self.save_path = save_path\n",
    "    args = Args()\n",
    "    args.include_wiggles = \"\" if args.include_wiggles == \"y\" else \"_nowiggles\"\n",
    "    \n",
    "    # 2. BAO fit initializer. This basically creates the path to load the results\n",
    "    baofit_initializer = BAOFitInitializer(\n",
    "        include_wiggles=args.include_wiggles,\n",
    "        dataset=args.dataset,\n",
    "        weight_type=args.weight_type,\n",
    "        mock_id=args.mock_id,\n",
    "        nz_flag=args.nz_flag,\n",
    "        cov_type=args.cov_type,\n",
    "        cosmology_template=args.cosmology_template,\n",
    "        cosmology_covariance=args.cosmology_covariance,\n",
    "        delta_theta=args.delta_theta,\n",
    "        theta_min=args.theta_min,\n",
    "        theta_max=args.theta_max,\n",
    "        pow_broadband=args.pow_broadband,\n",
    "        bins_removed=args.bins_removed,\n",
    "        alpha_min=args.alpha_min,\n",
    "        alpha_max=args.alpha_max,\n",
    "        verbose=False,\n",
    "        save_path=args.save_path,\n",
    "    )\n",
    "\n",
    "    fit_results[dataset, include_wiggles, weight_type, str(bins_removed)] = np.loadtxt(os.path.join(baofit_initializer.path_baofit, \"fit_results.txt\"))\n",
    "    likelihoods[dataset, include_wiggles, weight_type, str(bins_removed)] = np.loadtxt(os.path.join(baofit_initializer.path_baofit, \"likelihood_data.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5afee-9384-4884-bdc0-f1464e08e235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4657a9-4ec2-4e62-9071-f3d9bbc1e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmology_params = CosmologicalParameters(args.cosmology_template, verbose=True)\n",
    "cosmo = cosmology_params.get_cosmology()\n",
    "\n",
    "DM_fid = cosmo.comoving_angular_distance(0.85) / cosmo.h\n",
    "rd_fid = cosmo.rs_drag / cosmo.h\n",
    "factor = DM_fid / rd_fid\n",
    "\n",
    "systematic_error = np.sqrt(0.0035**2 + 0.0058**2)\n",
    "# systematic_error = np.sqrt(0.0045**2 + 0.0057**2)\n",
    "\n",
    "data = []\n",
    "for key, values in fit_results.items():\n",
    "    dataset, include_wiggles, weight_type, bins_removed = key\n",
    "    alpha, sigma_alpha, chi2, dof = values\n",
    "\n",
    "    total_sigma_alpha = np.sqrt(sigma_alpha**2 + systematic_error**2)\n",
    "    alpha_str = f\"${alpha:.4f} \\\\pm {total_sigma_alpha:.4f}$\"\n",
    "    dm_rd_str = f\"${alpha * factor:.2f} \\\\pm {total_sigma_alpha * factor:.2f}$\"\n",
    "    chi2_dof_str = f\"{chi2:.1f}/{int(round(dof))}\"\n",
    "\n",
    "    data.append([dataset, include_wiggles, weight_type, bins_removed, alpha_str, dm_rd_str, chi2_dof_str])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    \"Dataset\", \"Wiggles\", \"Weight Type\", \"Bins Removed\", \n",
    "    r\"$\\alpha \\pm \\sigma_\\alpha$\", r\"$D_M/r_d$\", r\"$\\chi^2/\\mathrm{dof}$\"\n",
    "])\n",
    "\n",
    "latex_table = df.to_latex(index=False, escape=False, column_format=\"ccc|ccc\")\n",
    "\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74be8d-9816-4afa-aad1-0249c9d1da7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0e511-e7d8-4341-9e93-34adc991629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the full sample\n",
    "alpha_full, sigma_alpha_full, _, _ = fit_results[\"DESY6\", \"y\", 1, \"[]\"]\n",
    "print(f\"[Full sample] α = {alpha_full:.4f} ± {sigma_alpha_full:.4f}\")\n",
    "\n",
    "# ---- DR1 tiles ----\n",
    "alpha_1, sigma_alpha_1, _, _ = fit_results[\"DESY6_DR1tiles_noDESI\", \"y\", 1, \"[]\"]\n",
    "alpha_2, sigma_alpha_2, _, _ = fit_results[\"DESY6_DR1tiles_DESIonly\", \"y\", 1, \"[]\"]\n",
    "\n",
    "# Combine\n",
    "alpha_combined = (alpha_1 / sigma_alpha_1**2 + alpha_2 / sigma_alpha_2**2) / (1 / sigma_alpha_1**2 + 1 / sigma_alpha_2**2)\n",
    "sigma_alpha_combined = (1 / (1 / sigma_alpha_1**2 + 1 / sigma_alpha_2**2))**0.5\n",
    "\n",
    "delta = alpha_combined - alpha_full\n",
    "delta_sigma = (sigma_alpha_combined**2 + sigma_alpha_full**2)**0.5\n",
    "significance = delta / delta_sigma\n",
    "\n",
    "print(f\"[DR1 tiles] Combined α = {alpha_combined:.4f} ± {sigma_alpha_combined:.4f}\")\n",
    "print(f\"Difference from full sample: Δα = {delta:.4f} ± {delta_sigma:.4f} ({significance:.2f}σ)\")\n",
    "\n",
    "# ---- Dec split ----\n",
    "alpha_1, sigma_alpha_1, _, _ = fit_results[\"DESY6_dec_below-23.5\", \"y\", 1, \"[]\"]\n",
    "alpha_2, sigma_alpha_2, _, _ = fit_results[\"DESY6_dec_above-23.5\", \"y\", 1, \"[]\"]\n",
    "\n",
    "# Combine\n",
    "alpha_combined = (alpha_1 / sigma_alpha_1**2 + alpha_2 / sigma_alpha_2**2) / (1 / sigma_alpha_1**2 + 1 / sigma_alpha_2**2)\n",
    "sigma_alpha_combined = (1 / (1 / sigma_alpha_1**2 + 1 / sigma_alpha_2**2))**0.5\n",
    "\n",
    "delta = alpha_combined - alpha_full\n",
    "delta_sigma = (sigma_alpha_combined**2 + sigma_alpha_full**2)**0.5\n",
    "significance = delta / delta_sigma\n",
    "\n",
    "print(f\"[Dec split] Combined α = {alpha_combined:.4f} ± {sigma_alpha_combined:.4f}\")\n",
    "print(f\"Difference from full sample: Δα = {delta:.4f} ± {delta_sigma:.4f} ({significance:.2f}σ)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077503cf-ea3e-466a-bff1-7097fd6b0549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c00034-cce5-471b-9a57-86a2601c7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def chi2_sigma_region(alpha, chi2, threshold=1):\n",
    "    best = np.argmin(chi2)\n",
    "    alpha_best = alpha[best]\n",
    "    chi2_best = chi2[best]\n",
    "\n",
    "    alpha_down = None\n",
    "    alpha_up = None\n",
    "\n",
    "    for i in range(best - 1, -1, -1):\n",
    "        if chi2[i] < chi2_best + threshold and chi2[i - 1] > chi2_best + threshold:\n",
    "            alpha_down = alpha[i]\n",
    "            break\n",
    "\n",
    "    for i in range(best, len(alpha) - 1):\n",
    "        if chi2[i] < chi2_best + threshold and chi2[i + 1] > chi2_best + threshold:\n",
    "            alpha_up = alpha[i]\n",
    "            break\n",
    "\n",
    "    if alpha_down is None or alpha_up is None:\n",
    "        raise ValueError(\"Could not find both threshold crossings.\")\n",
    "\n",
    "    sigma = (alpha_up - alpha_down) / 2\n",
    "\n",
    "    return alpha_best, sigma\n",
    "\n",
    "def chi2_shift_stretch(alpha, chi2, delta_alpha=0, sigma_alpha=1):\n",
    "    \n",
    "    # shift\n",
    "    \n",
    "    chi2_interp = scipy.interpolate.Akima1DInterpolator(alpha, chi2)\n",
    "    chi2_shift = chi2_interp(alpha - delta_alpha, extrapolate=False)\n",
    "    chi2_shift[np.isnan(chi2_shift)] = 10**10\n",
    "    \n",
    "    # stretch\n",
    "    \n",
    "    chi2_shift_interp = scipy.interpolate.Akima1DInterpolator(alpha, chi2_shift)\n",
    "    \n",
    "    alpha_mean = alpha[np.argmin(chi2_shift)]\n",
    "    \n",
    "    chi2_shift_stretch = chi2_shift_interp(sigma_alpha * (alpha - alpha_mean) + alpha_mean, extrapolate=False)\n",
    "    chi2_shift_stretch[np.isnan(chi2_shift_stretch)] = 10**10\n",
    "    \n",
    "    return chi2_shift_stretch\n",
    "\n",
    "# def chi2_shift_stretch(alpha, chi2, delta_alpha=0.0, sigma_alpha=1.0, nan_fill_value=None, method=\"pchip\"):\n",
    "\n",
    "#     # Select interpolator\n",
    "#     if method.lower() == \"pchip\":\n",
    "#         interpolator = scipy.interpolate.PchipInterpolator(alpha, chi2, extrapolate=(nan_fill_value is None))\n",
    "#     elif method.lower() == \"akima\":\n",
    "#         interpolator = scipy.interpolate.Akima1DInterpolator(alpha, chi2)\n",
    "#     else:\n",
    "#         raise ValueError(\"Method must be 'pchip' or 'akima'\")\n",
    "\n",
    "#     # Apply shift and stretch in alpha-space\n",
    "#     alpha_shifted = alpha - delta_alpha\n",
    "#     alpha_min_chi2 = alpha_shifted[np.argmin(interpolator(alpha_shifted))]\n",
    "#     alpha_transformed = sigma_alpha * (alpha_shifted - alpha_min_chi2) + alpha_min_chi2\n",
    "\n",
    "#     # Interpolate transformed curve\n",
    "#     chi2_transformed = interpolator(alpha_transformed)\n",
    "\n",
    "#     # Handle NaNs\n",
    "#     if nan_fill_value is not None:\n",
    "#         chi2_transformed = np.where(np.isnan(chi2_transformed), nan_fill_value, chi2_transformed)\n",
    "\n",
    "#     return chi2_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43900e77-a14e-4ef3-8bf9-883c98303874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb446078-08db-4897-8ad0-847536e5b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [\n",
    "    \"DESY6\", \n",
    "    \"DESY6_DR1tiles_noDESI\", \n",
    "    \"DESY6_dec_below-23.5\"\n",
    "]\n",
    "\n",
    "color_dict = {\n",
    "    \"DESY6\": \"mediumblue\",\n",
    "    \"DESY6_DR1tiles_noDESI\": \"#197D19\",\n",
    "    \"DESY6_DR1tiles_DESIonly\": \"mediumpurple\",\n",
    "    \"DESY6_dec_below-23.5\": \"orange\",\n",
    "    \"DESY6_dec_above-23.5\": \"red\"\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    alpha_vector, chi2_vector_orig = likelihoods[dataset, \"y\", 1, \"[]\"].T\n",
    "    # ax.plot(alpha_vector, chi2_vector_orig - chi2_vector_orig.min(), color=color_dict.get(dataset, \"black\"), label=fr\"rework_dataset_name(dataset)}\")\n",
    "\n",
    "    alpha_orig, sigma_alpha_orig = chi2_sigma_region(alpha_vector, chi2_vector_orig - chi2_vector_orig.min())\n",
    "    print([dataset, f\"{alpha_orig:.4f}\", f\"{sigma_alpha_orig:.4f}\", f\"{sigma_alpha_orig:.4f}\"])\n",
    "\n",
    "    alpha_vector_ss = np.copy(alpha_vector)\n",
    "    chi2_vector_ss = chi2_shift_stretch(alpha_vector, chi2_vector_orig - chi2_vector_orig.min(), sigma_alpha=sigma_alpha_orig / np.sqrt(sigma_alpha_orig**2 + systematic_error**2))\n",
    "    chi2_vector_ss += chi2_vector_orig.min()\n",
    "    \n",
    "    alpha_vector_ss = alpha_vector_ss[np.concatenate(np.argwhere(chi2_vector_ss < 10**3))]\n",
    "    chi2_vector_ss = chi2_vector_ss[np.concatenate(np.argwhere(chi2_vector_ss < 10**3))]\n",
    "    \n",
    "    ax.plot(alpha_vector_ss, chi2_vector_ss - chi2_vector_ss.min(), color=color_dict.get(dataset, \"black\"), label=fr\"{rework_dataset_name(dataset)}\")\n",
    "\n",
    "    alpha_ss, sigma_alpha_ss = chi2_sigma_region(alpha_vector_ss, chi2_vector_ss - chi2_vector_ss.min())\n",
    "    print([dataset, f\"{alpha_ss:.4f}\", f\"{sigma_alpha_ss:.4f}\", f\"{np.sqrt(sigma_alpha_orig**2 + systematic_error**2):.4f}\"])\n",
    "    \n",
    "    alpha_nw, chi2_nw = likelihoods[dataset, \"n\", 1, \"[]\"].T\n",
    "    ax.plot(alpha_nw, chi2_nw - chi2_vector_orig.min(), color=color_dict.get(dataset, \"black\"), linestyle=\"--\")\n",
    "\n",
    "    np.savetxt(\n",
    "        f\"plots/likelihood_{dataset}.csv\",\n",
    "        np.column_stack([alpha_vector_ss, chi2_vector_ss]),\n",
    "        delimiter=\",\",\n",
    "        header=\"alpha,chi2\",\n",
    "        comments=\"\",  # avoids '#' in header\n",
    "        fmt=\"%.6e\"    # scientific notation, adjust as needed\n",
    "    )\n",
    "\n",
    "# Horizontal lines\n",
    "for n in np.arange(1, 6):\n",
    "    y = n**2\n",
    "    ax.axhline(y, color=\"grey\", linestyle=\"--\")\n",
    "    ax.text(1.21, y, rf\"{n}$\\sigma$\", va='center', ha='left', fontsize=15, color='black')\n",
    "\n",
    "# Axis limits\n",
    "ax.set_xlim([0.8, 1.2])\n",
    "\n",
    "# Labels and ticks\n",
    "ax.set_xlabel(r\"$\\alpha$\", fontsize=20)\n",
    "ax.set_ylabel(r\"$\\chi^2$\", fontsize=20)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(5))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(5))\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "\n",
    "ax.legend(fontsize=13)\n",
    "\n",
    "plt.savefig(\"plots/likelihoods_all.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d84de-87fc-4efc-9997-5d60cd7506b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b6b0e-3839-40ce-9bf5-bb94fea97816",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [\"DESY6\", \"DESY6_DR1tiles_noDESI\", \"DESY6_dec_below-23.5\"]\n",
    "include_wiggles = \"y\"\n",
    "weight_type = 1\n",
    "bins_removed_list = [[1, 2, 3, 4, 5], [0, 2, 3, 4, 5], [0, 1, 3, 4, 5], [0, 1, 2, 4, 5], [0, 1, 2, 3, 5], [0, 1, 2, 3, 4]]\n",
    "\n",
    "values = [0.65 + 0.1 * i for i in range(len(bins_removed_list))]\n",
    "z_dict = {str(bins_removed): val for bins_removed, val in zip(bins_removed_list, values)}\n",
    "\n",
    "color_dict = {\n",
    "    \"DESY6\": \"mediumblue\",\n",
    "    \"DESY6_DR1tiles_noDESI\": \"#197D19\",\n",
    "    \"DESY6_DR1tiles_DESIonly\": \"mediumpurple\",\n",
    "    \"DESY6_dec_below-23.5\": \"orange\",\n",
    "    \"DESY6_dec_above-23.5\": \"red\"\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i_dataset, dataset in enumerate(dataset_list):\n",
    "    leg_cont = 0\n",
    "    x_shift = 0.005 * i_dataset\n",
    "    for bins_removed in bins_removed_list:\n",
    "        alpha, sigma_alpha, chi2, dof = fit_results[dataset, include_wiggles, weight_type, str(bins_removed)].T\n",
    "\n",
    "        if sigma_alpha < 1e3:\n",
    "            ax.errorbar(\n",
    "                z_dict[str(bins_removed)] + x_shift,\n",
    "                alpha,\n",
    "                sigma_alpha,\n",
    "                capsize=3,\n",
    "                fmt=\".\",\n",
    "                color=color_dict.get(dataset, \"black\"),\n",
    "                label=fr\"{rework_dataset_name(dataset)}\" if leg_cont == 0 else None  # avoid duplicate legend entries\n",
    "            )\n",
    "            leg_cont = 1\n",
    "\n",
    "ax.set_ylim([0.83, 1.17])\n",
    "\n",
    "ax.set_xlabel(r\"$z$\", fontsize=20)\n",
    "ax.set_ylabel(r\"$\\alpha$\", fontsize=20)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(5))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(5))\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "\n",
    "ax.axhline(y=1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "ax.legend(fontsize=13)\n",
    "\n",
    "# Add labels for bins\n",
    "for i, bins_removed in enumerate(bins_removed_list):\n",
    "    x_pos = z_dict[str(bins_removed)]\n",
    "    if i != 0:\n",
    "        ax.text(\n",
    "            x_pos,\n",
    "            1.175,  # slightly above the y-limit (adjust if needed)\n",
    "            fr\"bin {i+1}\",  # Labels start from bin 2\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=15\n",
    "        )\n",
    "\n",
    "plt.savefig(\"plots/bao_individual_bins.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# bins_removed_list = [[]]\n",
    "\n",
    "# for i_dataset, dataset in enumerate(dataset_list):\n",
    "#     x_shift = 0.005 * i_dataset\n",
    "#     for bins_removed in bins_removed_list:\n",
    "#         alpha, sigma_alpha, chi2, dof = fit_results[dataset, include_wiggles, weight_type, str(bins_removed)].T\n",
    "\n",
    "#         if sigma_alpha < 1e3:\n",
    "#             ax.errorbar(\n",
    "#                 0.85 + x_shift,\n",
    "#                 alpha,\n",
    "#                 sigma_alpha,\n",
    "#                 capsize=3,\n",
    "#                 fmt=\"d\",\n",
    "#                 color=color_dict.get(dataset, \"black\"),\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c5960-37f8-47ca-bc89-097a2ccfdad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633d269-5348-4adb-b942-06c90e55987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosmology_params_planck = CosmologicalParameters(\"planck\", verbose=True)\n",
    "# cosmo_planck = cosmology_params_planck.get_cosmology()\n",
    "\n",
    "# DM_planck = cosmo_planck.comoving_angular_distance(0.85) / cosmo_planck.h\n",
    "# rd_planck = cosmo_planck.rs_drag / cosmo_planck.h\n",
    "# factor_planck = DM_planck / rd_planck\n",
    "\n",
    "\n",
    "# cosmology_params_mice = CosmologicalParameters(\"mice\", verbose=True)\n",
    "# cosmo_mice = cosmology_params_mice.get_cosmology()\n",
    "\n",
    "# DM_mice = cosmo_mice.comoving_angular_distance(0.85) / cosmo_mice.h\n",
    "# rd_mice = cosmo_mice.rs_drag / cosmo_mice.h\n",
    "# factor_mice = DM_mice / rd_mice\n",
    "\n",
    "\n",
    "# print(factor_mice / factor_planck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f3339-73ac-4bc0-b502-cb076eec08ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi-main",
   "language": "python",
   "name": "cosmodesi-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
