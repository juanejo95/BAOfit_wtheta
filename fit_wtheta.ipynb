{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756b5e32-927f-45c1-af60-67a229960586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fid n(z), which has 6 redshift bins\n",
      "This should be 0: 2.7755575615628914e-17\n",
      "[0.9513, 0.0282, 94.7796, 119]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy.stats import chi2\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Argument parser setup\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--args.cov_type', help='mocks or cosmolike', default='mocks', type=str)\n",
    "# parser.add_argument('--args.cosmology_template', help='mice, planck or lognormal_Y6BAO', default='mice', type=str)\n",
    "# parser.add_argument('--args.cosmology_covariance', help='mice, planck or lognormal_Y6BAO', default='mice', type=str)\n",
    "# parser.add_argument('--args.diag_only', help='y or n', default='n', type=str)\n",
    "# parser.add_argument('--args.remove_crosscov', help='y or n', default='n', type=str)\n",
    "# parser.add_argument('--args.delta_theta', help='Values: 0.05, 0.1, 0.15 or 0.2', default=0.2, type=float)\n",
    "# parser.add_argument('--args.theta_min', help='min theta to do the BAO fit', default=0.5, type=float)\n",
    "# parser.add_argument('--args.theta_max', help='max theta to do the BAO fit', default=5, type=float)\n",
    "# parser.add_argument('--args.n_broadband', help='1, 2, 3, 4 or 5', default=3, type=int)\n",
    "# parser.add_argument('--args.bins_removed', help='bins removed when doing the BAO fit', default='None', type=str)\n",
    "# parser.add_argument('--args.nz_flag', help='original or modified (shifted+stretched)', default='fid', type=str)\n",
    "# parser.add_argument('--args.include_wiggles', help='y or n', default='y', type=str)\n",
    "# parser.add_argument('--args.weight_type', help='Only allowed if fit_data=1. 0 for no weights, 1 for systematic weights applied', default=1, type=int)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.cov_type = \"cosmolike_data\"\n",
    "        self.cosmology_template = \"planck\"\n",
    "        self.cosmology_covariance = \"planck\"\n",
    "        self.diag_only = \"n\"\n",
    "        self.remove_crosscov = \"n\"\n",
    "        self.delta_theta = 0.2\n",
    "        self.theta_min = 0\n",
    "        self.theta_max = 5\n",
    "        self.n_broadband = 3\n",
    "        self.bins_removed = 'None'\n",
    "        self.nz_flag = \"fid\"\n",
    "        self.include_wiggles = \"y\"\n",
    "        self.weight_type = 1\n",
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.cov_type = \"cosmolike_data\"\n",
    "#         self.cosmology_template = \"planck\"\n",
    "#         self.cosmology_covariance = \"planck\"\n",
    "#         self.diag_only = \"n\"\n",
    "#         self.remove_crosscov = \"n\"\n",
    "#         self.delta_theta = 0.2\n",
    "#         self.theta_min = 0\n",
    "#         self.theta_max = 5\n",
    "#         self.n_broadband = 6\n",
    "#         self.bins_removed = 'None'\n",
    "#         self.nz_flag = \"fid_5\"\n",
    "#         self.include_wiggles = \"n\"\n",
    "#         self.weight_type = 1\n",
    "args = Args()\n",
    "args.include_wiggles = '' if args.include_wiggles == 'y' else '_noBAO'\n",
    "\n",
    "# Limits for the fits\n",
    "alpha_min_allowed = 0.8\n",
    "alpha_max_allowed = 1.2\n",
    "\n",
    "# Handling args.bins_removed argument\n",
    "bin_mappings = {\n",
    "    'None': [],\n",
    "    '0': [0], '1': [1], '2': [2], '3': [3], '4': [4], '5': [5],\n",
    "    '25': [2, 5], '12345': [1, 2, 3, 4, 5], '02345': [0, 2, 3, 4, 5],\n",
    "    '01345': [0, 1, 3, 4, 5], '01245': [0, 1, 2, 4, 5], '01235': [0, 1, 2, 3, 5], '0125': [0, 1, 2, 5],\n",
    "    '01234': [0, 1, 2, 3, 4], '012': [0, 1, 2], '345': [3, 4, 5], '45': [4, 5]\n",
    "}\n",
    "args.bins_removed = bin_mappings.get(args.bins_removed, args.bins_removed)\n",
    "\n",
    "# Galaxy bias dictionary\n",
    "galaxy_bias = {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}\n",
    "\n",
    "# Redshift distributions\n",
    "from utils import redshift_distributions\n",
    "nz_instance = redshift_distributions(args.nz_flag)\n",
    "\n",
    "# Setting up save directory\n",
    "savedir = (\n",
    "    f\"fit_results{args.include_wiggles}/data_{args.weight_type}/nz{args.nz_flag}_cov{args.cov_type}_\"\n",
    "    f\"{args.cosmology_template}temp_{args.cosmology_covariance}cov_deltatheta{args.delta_theta}_\"\n",
    "    f\"thetamin{args.theta_min}_{args.n_broadband}broadband_binsremoved{args.bins_removed}\"\n",
    ")\n",
    "\n",
    "# # Check if the results already exist\n",
    "# if os.path.exists(f\"{savedir}/alpha_results_data.txt\"):\n",
    "#     print(\"This one already done, moving to the next one!\")\n",
    "#     sys.exit()\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "# Load the covariance matrix\n",
    "if args.cov_type == 'cosmolike_data':\n",
    "    if args.cosmology_covariance == 'mice':\n",
    "        if args.delta_theta != 0.2:\n",
    "            print(f\"No mice cosmolike covariance matrix for delta_theta={args.delta_theta}\")\n",
    "            sys.exit()\n",
    "        cov = np.loadtxt(\n",
    "            f\"cov_cosmolike/cov_Y6bao_data_DeltaTheta{str(args.delta_theta).replace('.', 'p')}_mask_g_mice.txt\"\n",
    "        )\n",
    "    elif args.cosmology_covariance == 'planck':\n",
    "        cov = np.loadtxt(\n",
    "            f\"cov_cosmolike/cov_Y6bao_data_DeltaTheta{str(args.delta_theta).replace('.', 'p')}_mask_g_planck.txt\"\n",
    "        )\n",
    "    theta_cov = np.loadtxt(f\"cov_cosmolike/delta_theta_{args.delta_theta}_binning.txt\")[:, 2] * np.pi / 180\n",
    "\n",
    "# Adjust covariance matrix for removed bins\n",
    "cov_adjusted = np.zeros_like(cov)\n",
    "for bin_z1 in range(nz_instance.nbins):\n",
    "    for bin_z2 in range(nz_instance.nbins):\n",
    "        slice_1 = slice(bin_z1 * len(theta_cov), (bin_z1 + 1) * len(theta_cov))\n",
    "        slice_2 = slice(bin_z2 * len(theta_cov), (bin_z2 + 1) * len(theta_cov))\n",
    "        if bin_z1 == bin_z2 or (bin_z1 not in args.bins_removed and bin_z2 not in args.bins_removed):\n",
    "            cov_adjusted[slice_1, slice_2] = cov[slice_1, slice_2]\n",
    "cov = cov_adjusted\n",
    "\n",
    "# Remove cross-covariances\n",
    "if args.remove_crosscov == 'y':\n",
    "    cov_adjusted = np.zeros_like(cov)\n",
    "    for bin_z in range(nz_instance.nbins):\n",
    "        slice_ = slice(bin_z * len(theta_cov), (bin_z + 1) * len(theta_cov))\n",
    "        cov_adjusted[slice_, slice_] = cov[slice_, slice_]\n",
    "    cov = cov_adjusted\n",
    "\n",
    "# Retain only the diagonal\n",
    "if args.diag_only == 'y':\n",
    "    cov = np.diag(np.diag(cov))\n",
    "\n",
    "cov_orig = np.copy(cov)\n",
    "\n",
    "# Extend theta covariance across all bins\n",
    "theta_cov = np.concatenate([theta_cov] * nz_instance.nbins)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the data and theoretical wtheta\n",
    "indices = {}\n",
    "indices_2 = {}\n",
    "\n",
    "theta_wtheta_data = {}\n",
    "wtheta_data = {}\n",
    "\n",
    "theta_wtheta_th = {}\n",
    "wtheta_th = {}\n",
    "\n",
    "wtheta_bb_th = {}\n",
    "wtheta_bf_th = {}\n",
    "wtheta_ff_th = {}\n",
    "\n",
    "err_wtheta_data = {}\n",
    "\n",
    "for bin_z in range(nz_instance.nbins):\n",
    "    filename_wtheta = f'wtheta_data_Yband/wtheta_data_bin{bin_z}_DeltaTheta{args.delta_theta}_weights{args.weight_type}_fstar.txt'\n",
    "    theta, wtheta = np.loadtxt(filename_wtheta).T\n",
    "    \n",
    "    ind_theta = np.where((theta > args.theta_min * np.pi / 180) & (theta < args.theta_max * np.pi / 180))[0]\n",
    "\n",
    "    theta_wtheta_data[bin_z] = theta[ind_theta]\n",
    "    wtheta_data[bin_z] = wtheta[ind_theta]\n",
    "\n",
    "    templates_base = f'wtheta_template{args.include_wiggles}/nz_{args.nz_flag}/wtheta_{args.cosmology_template}'\n",
    "    theta_wtheta_th[bin_z] = np.loadtxt(f'{templates_base}/wtheta_bb_bin{bin_z}.txt')[:, 0]\n",
    "    wtheta_bb_th[bin_z] = np.loadtxt(f'{templates_base}/wtheta_bb_bin{bin_z}.txt')[:, 1]\n",
    "    wtheta_bf_th[bin_z] = np.loadtxt(f'{templates_base}/wtheta_bf_bin{bin_z}.txt')[:, 1]\n",
    "    wtheta_ff_th[bin_z] = np.loadtxt(f'{templates_base}/wtheta_ff_bin{bin_z}.txt')[:, 1]\n",
    "\n",
    "    wtheta_th[bin_z] = (\n",
    "        galaxy_bias[bin_z]**2 * wtheta_bb_th[bin_z] +\n",
    "        galaxy_bias[bin_z] * wtheta_bf_th[bin_z] +\n",
    "        wtheta_ff_th[bin_z]\n",
    "    )\n",
    "\n",
    "    if bin_z == 0:\n",
    "        indices[bin_z] = [0, len(ind_theta)]\n",
    "    else:\n",
    "        indices[bin_z] = [indices[bin_z - 1][1], indices[bin_z - 1][1] + len(ind_theta)]\n",
    "\n",
    "    indices_2[bin_z] = ind_theta + bin_z * len(theta)\n",
    "\n",
    "    err_wtheta_data[bin_z] = np.sqrt(np.diag(cov_orig))[indices_2[bin_z]]\n",
    "\n",
    "# Interpolate the theoretical wtheta to use it as the template\n",
    "wtheta_th_interp = {}\n",
    "wtheta_bb_th_interp = {}\n",
    "wtheta_bf_th_interp = {}\n",
    "wtheta_ff_th_interp = {}\n",
    "\n",
    "for bin_z in range(nz_instance.nbins):\n",
    "    wtheta_th_interp[bin_z] = interp1d(theta_wtheta_th[bin_z], wtheta_th[bin_z], kind='cubic')\n",
    "    wtheta_bb_th_interp[bin_z] = interp1d(theta_wtheta_th[bin_z], wtheta_bb_th[bin_z], kind='cubic')\n",
    "    wtheta_bf_th_interp[bin_z] = interp1d(theta_wtheta_th[bin_z], wtheta_bf_th[bin_z], kind='cubic')\n",
    "    wtheta_ff_th_interp[bin_z] = interp1d(theta_wtheta_th[bin_z], wtheta_ff_th[bin_z], kind='cubic')\n",
    "\n",
    "# Prepare the covariance matrix\n",
    "indices_fit = np.concatenate([indices_2[i] for i in range(nz_instance.nbins)])\n",
    "cov = cov_orig[indices_fit[:, None], indices_fit]\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "# Concatenate the datavector\n",
    "theta_wtheta_data_concatenated = np.concatenate([theta_wtheta_data[i] for i in range(nz_instance.nbins)])\n",
    "\n",
    "wtheta_data_concatenated = []\n",
    "for bin_z in range(nz_instance.nbins):\n",
    "    if bin_z in args.bins_removed:\n",
    "        wtheta_data_concatenated.append(np.zeros(len(wtheta_data[bin_z])))\n",
    "    else:\n",
    "        wtheta_data_concatenated.append(wtheta_data[bin_z])\n",
    "wtheta_data_concatenated = np.concatenate(wtheta_data_concatenated)\n",
    "\n",
    "err_wtheta_data_concatenated = np.concatenate([err_wtheta_data[i] for i in range(nz_instance.nbins)])\n",
    "\n",
    "# Verify consistency between theta from covariance and data\n",
    "print('This should be 0:', abs(theta_wtheta_data_concatenated - theta_cov[indices_fit]).max())\n",
    "if abs(theta_wtheta_data_concatenated - theta_cov[indices_fit]).max() > 1e-5:\n",
    "    print('The covariance matrix and the wtheta of the mocks do not have the same theta')\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameter names. Max 6 broadband terms, max 6 redshift bins. Can be adjusted\n",
    "name = np.array([\n",
    "    'alpha',\n",
    "    'A_0', 'B_0', 'C_0', 'D_0', 'E_0', 'F_0', 'G_0',\n",
    "    'A_1', 'B_1', 'C_1', 'D_1', 'E_1', 'F_1', 'G_1',\n",
    "    'A_2', 'B_2', 'C_2', 'D_2', 'E_2', 'F_2', 'G_2',\n",
    "    'A_3', 'B_3', 'C_3', 'D_3', 'E_3', 'F_3', 'G_3',\n",
    "    'A_4', 'B_4', 'C_4', 'D_4', 'E_4', 'F_4', 'G_4',\n",
    "    'A_5', 'B_5', 'C_5', 'D_5', 'E_5', 'F_5', 'G_5',\n",
    "])\n",
    "\n",
    "n_broadband_max = int((len(name) - 1)/6 - 1) # this should be 6\n",
    "n_params_max = len(name)\n",
    "\n",
    "# Initial parameter guesses and bounds\n",
    "p0_list = [1]\n",
    "for _ in range(nz_instance.nbins):\n",
    "    row = [1] + [0] * (n_broadband_max)\n",
    "    p0_list.extend(row)\n",
    "\n",
    "p0 = np.array(p0_list)\n",
    "\n",
    "bounds = (\n",
    "    (alpha_min_allowed, *[0, -1, -1, -1, -1, -1, -1] * nz_instance.nbins),\n",
    "    (alpha_max_allowed, *[15, 1, 1, 1, 1, 1, 1] * nz_instance.nbins)\n",
    ")\n",
    "\n",
    "IND = np.arange(1, 1 + (args.n_broadband + 1))\n",
    "IND2 = np.concatenate([[0], IND])\n",
    "\n",
    "for k in range(1, nz_instance.nbins):\n",
    "    IND2 = np.concatenate([IND2, IND + k * (1 + n_broadband_max)])\n",
    "\n",
    "name = name[IND2]\n",
    "p0 = p0[IND2]\n",
    "bounds = (\n",
    "    list(np.array(bounds[0])[IND2]),\n",
    "    list(np.array(bounds[1])[IND2])\n",
    ")\n",
    "\n",
    "# Calculate effective parameter counts\n",
    "n_params = len(p0)\n",
    "n_params_eff = len(p0) - (1 + args.n_broadband) * len(args.bins_removed)\n",
    "\n",
    "# Define the theoretical template\n",
    "\n",
    "def wtheta_template_raw(x, alpha, *params):\n",
    "    x_sections = [theta_wtheta_data_concatenated[indices[i][0]:indices[i][1]] for i in range(nz_instance.nbins)]\n",
    "\n",
    "    for i, x_i in enumerate(x_sections):\n",
    "        if (alpha * x_i).min() < theta_wtheta_th[i].min() or \\\n",
    "           (alpha * x_i).max() > theta_wtheta_th[i].max():\n",
    "            print('Bad news: the template is being extrapolated')\n",
    "            print(alpha)\n",
    "\n",
    "    v = np.concatenate([\n",
    "        params[(1 + n_broadband_max) * i] * wtheta_th_interp[i](alpha * x_sections[i]) + # constant that multiplies the template (A)\n",
    "        params[(1 + n_broadband_max) * i + 1] + # constant (B)\n",
    "        params[(1 + n_broadband_max) * i + 2] / x_sections[i] + # 1/theta term (C)\n",
    "        params[(1 + n_broadband_max) * i + 3] / x_sections[i]**2 + # 1/theta**2 term (D)\n",
    "        params[(1 + n_broadband_max) * i + 4] * x_sections[i] + # theta term (E)\n",
    "        params[(1 + n_broadband_max) * i + 5] * x_sections[i]**2 + # theta**2 term (F)\n",
    "        params[(1 + n_broadband_max) * i + 6] * x_sections[i]**3 # theta**3 term (G)\n",
    "        for i in range(nz_instance.nbins)\n",
    "    ])\n",
    "\n",
    "    return v\n",
    "\n",
    "def get_wtheta_template(n_broadband, n_params_max, IND2):\n",
    "    param_names = []\n",
    "    for i in range(args.n_broadband + 1):\n",
    "        for letter in 'ABCDEFG':\n",
    "            param_names.append(f\"{letter}_{i}\")\n",
    "\n",
    "    def wtheta_template(x, *args):\n",
    "        pars = np.zeros(n_params_max)\n",
    "        pars[IND2] = args\n",
    "        return wtheta_template_raw(x, *pars)\n",
    "    \n",
    "    return wtheta_template\n",
    "\n",
    "wtheta_template = get_wtheta_template(args.n_broadband, n_params_max, IND2)\n",
    "\n",
    "# First fit attempt. We use curve_fit\n",
    "\n",
    "popt, pcov = curve_fit(wtheta_template, theta_wtheta_data_concatenated, wtheta_data_concatenated, p0=p0, bounds=bounds, sigma=cov, absolute_sigma=False)\n",
    "err_params = np.sqrt(np.diag(pcov))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let's compute the chi-squared statistic\n",
    "wtheta_fit_concatenated = wtheta_template(theta_wtheta_data_concatenated, *popt)\n",
    "diff_mean = wtheta_data_concatenated - wtheta_fit_concatenated\n",
    "chi_square = diff_mean @ inv_cov @ diff_mean\n",
    "\n",
    "def least_squares(params):\n",
    "    y_th = wtheta_template(theta_wtheta_data_concatenated, *params)\n",
    "    diff = wtheta_data_concatenated - y_th\n",
    "    return diff @ inv_cov @ diff\n",
    "\n",
    "# Initialize the model matrix MM\n",
    "MM = np.zeros([len(theta_wtheta_data_concatenated), nz_instance.nbins * args.n_broadband])\n",
    "\n",
    "# Positions of the amplitude parameters in the model function\n",
    "pos_amplitude = np.array([1 + i * (args.n_broadband + 1) for i in np.arange(0, nz_instance.nbins)])\n",
    "\n",
    "# Positions of the broadband parameters in the model function\n",
    "pos_broadband = np.delete(np.arange(0, n_params), np.concatenate(([0], pos_amplitude)))\n",
    "\n",
    "# Construct the model matrix MM\n",
    "for j in np.arange(0, nz_instance.nbins * args.n_broadband):\n",
    "    fit_params = np.zeros(n_params)\n",
    "    fit_params[0] = 1  # The value of alpha doesn't matter here since the amplitude is 0\n",
    "    fit_params[pos_broadband[j]] = 1\n",
    "    MM[:, j] = wtheta_template(theta_wtheta_data_concatenated, *fit_params)\n",
    "\n",
    "# Compute the pseudo-inverse of MM\n",
    "MM_2 = np.linalg.inv((MM.T) @ inv_cov @ MM) @ (MM.T) @ inv_cov\n",
    "\n",
    "\n",
    "def broadband_params(amplitude_params, alpha):\n",
    "    fit_params = np.zeros(n_params)\n",
    "    fit_params[0] = alpha\n",
    "    fit_params[pos_amplitude] = amplitude_params\n",
    "    return MM_2 @ (wtheta_data_concatenated - wtheta_template(theta_wtheta_data_concatenated, *fit_params))\n",
    "\n",
    "\n",
    "def least_squares_2(amplitude_params, alpha):\n",
    "    fit_params = np.zeros(n_params)\n",
    "    fit_params[0] = alpha\n",
    "    fit_params[pos_amplitude] = amplitude_params\n",
    "    fit_params[pos_broadband] = broadband_params(amplitude_params, alpha)\n",
    "\n",
    "    vector_ones = np.ones(nz_instance.nbins)\n",
    "    vector_ones[args.bins_removed] = 0\n",
    "\n",
    "    return least_squares(fit_params) + np.sum(((amplitude_params - vector_ones) / 0.4) ** 2)\n",
    "\n",
    "\n",
    "# Tolerance for the minimization procedure\n",
    "tol_minimize = 1e-7\n",
    "\n",
    "# Search for the alpha that minimizes the chi-squared over a grid from alpha_min_allowed to alpha_max_allowed\n",
    "n = 2 * 10**2\n",
    "alpha_vector = np.linspace(alpha_min_allowed, alpha_max_allowed, n)\n",
    "chi2_vector = np.zeros(n)\n",
    "\n",
    "for i in np.arange(0, n):\n",
    "    amplitude_params = minimize(least_squares_2, x0=np.ones(nz_instance.nbins), method='SLSQP', bounds=([(0, None)] * nz_instance.nbins), \n",
    "                                tol=tol_minimize, args=(alpha_vector[i]))\n",
    "    chi2_vector[i] = least_squares_2(amplitude_params.x, alpha_vector[i])\n",
    "\n",
    "# Find the best alpha value\n",
    "best = np.argmin(chi2_vector)\n",
    "alpha_best = alpha_vector[best]\n",
    "chi2_best = chi2_vector[best]\n",
    "\n",
    "# Set chi-square to the best chi-squared value\n",
    "chi_square = chi2_best\n",
    "\n",
    "# Save the likelihood data for '_noBAO' condition\n",
    "if args.include_wiggles == '_noBAO':\n",
    "    np.savetxt(savedir + '/likelihood_data.txt', np.column_stack([alpha_vector, chi2_vector]))\n",
    "\n",
    "elif args.include_wiggles == '':\n",
    "    if chi2_vector[0] > chi2_best + 1 and chi2_vector[-1] > chi2_best + 1:\n",
    "        \n",
    "        for i in np.arange(0, best)[::-1]:\n",
    "            if chi2_vector[i] < chi2_best + 1 and chi2_vector[i - 1] > chi2_best + 1:\n",
    "                alpha_down = alpha_vector[i]\n",
    "                break\n",
    "\n",
    "        for i in np.arange(best, n):\n",
    "            if chi2_vector[i] < chi2_best + 1 and chi2_vector[i + 1] > chi2_best + 1:\n",
    "                alpha_up = alpha_vector[i]\n",
    "                break\n",
    "\n",
    "        alpha_best_vector = np.linspace(alpha_best - 0.01, alpha_best + 0.01, n)\n",
    "        chi2_best_vector = np.zeros(n)\n",
    "        alpha_down_vector = np.linspace(alpha_down - 0.01, alpha_down + 0.01, n)\n",
    "        chi2_down_vector = np.zeros(n)\n",
    "        alpha_up_vector = np.linspace(alpha_up - 0.01, alpha_up + 0.01, n)\n",
    "        chi2_up_vector = np.zeros(n)\n",
    "\n",
    "        for i in np.arange(0, n):\n",
    "            amplitude_params_best = minimize(least_squares_2, x0=np.ones(nz_instance.nbins), method='SLSQP', bounds=([(0, None)] * nz_instance.nbins),\n",
    "                                             tol=tol_minimize, args=(alpha_best_vector[i]))\n",
    "            chi2_best_vector[i] = least_squares_2(amplitude_params_best.x, alpha_best_vector[i])\n",
    "\n",
    "            amplitude_params_down = minimize(least_squares_2, x0=np.ones(nz_instance.nbins), method='SLSQP', bounds=([(0, None)] * nz_instance.nbins),\n",
    "                                             tol=tol_minimize, args=(alpha_down_vector[i]))\n",
    "            chi2_down_vector[i] = least_squares_2(amplitude_params_down.x, alpha_down_vector[i])\n",
    "\n",
    "            amplitude_params_up = minimize(least_squares_2, x0=np.ones(nz_instance.nbins), method='SLSQP', bounds=([(0, None)] * nz_instance.nbins),\n",
    "                                           tol=tol_minimize, args=(alpha_up_vector[i]))\n",
    "            chi2_up_vector[i] = least_squares_2(amplitude_params_up.x, alpha_up_vector[i])\n",
    "\n",
    "        best_new = np.argmin(chi2_best_vector)\n",
    "        alpha_best_new = alpha_best_vector[best_new]\n",
    "        chi2_best_new = chi2_best_vector[best_new]\n",
    "\n",
    "        alpha_down_new = alpha_down_vector[np.argmin(abs(chi2_down_vector - (chi2_best + 1)))]\n",
    "        alpha_up_new = alpha_up_vector[np.argmin(abs(chi2_up_vector - (chi2_best + 1)))]\n",
    "        \n",
    "        if alpha_best_vector[0] < alpha_best_new < alpha_best_vector[-1] and \\\n",
    "           alpha_down_vector[0] < alpha_down_new < alpha_down_vector[-1] and \\\n",
    "           alpha_up_vector[0] < alpha_up_new < alpha_up_vector[-1]:\n",
    "            alpha_best, alpha_down, alpha_up = alpha_best_new, alpha_down_new, alpha_up_new\n",
    "            chi_square = chi2_best_new\n",
    "        else:\n",
    "            print('There is a problem with the fit!')\n",
    "            good_fit = 1\n",
    "\n",
    "        amplitude_params = minimize(least_squares_2, x0=np.ones(nz_instance.nbins), method='SLSQP', bounds=([(0, None)] * nz_instance.nbins),\n",
    "                                    tol=tol_minimize, args=(alpha_best))\n",
    "\n",
    "        popt = np.zeros(n_params)\n",
    "        popt[0] = alpha_best\n",
    "        popt[pos_amplitude] = amplitude_params.x\n",
    "        popt[pos_broadband] = broadband_params(amplitude_params.x, alpha_best)\n",
    "\n",
    "        err_alpha_down = alpha_best - alpha_down\n",
    "        err_alpha_up = alpha_up - alpha_best\n",
    "        err_alpha = (alpha_up - alpha_down) / 2\n",
    "        err_params[0] = err_alpha\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.plot(alpha_vector, chi2_vector)\n",
    "        plt.plot(alpha_best, chi2_best, 'd')\n",
    "        plt.plot(alpha_vector, np.ones(n) * (chi2_best + 1), '--r')\n",
    "        plt.plot((alpha_best - err_alpha_down) * np.ones(n), np.linspace(chi2_vector.min(), chi2_vector.max(), n), '--k')\n",
    "        plt.plot((alpha_best + err_alpha_up) * np.ones(n), np.linspace(chi2_vector.min(), chi2_vector.max(), n), '--k')\n",
    "        plt.xlabel(r'$\\alpha$', fontsize=14)\n",
    "        plt.ylabel(r'$\\chi^2$', fontsize=14)\n",
    "        plt.savefig(savedir + '/fig.png', bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        np.savetxt(savedir + '/likelihood_data.txt', np.column_stack([alpha_vector, chi2_vector]))\n",
    "\n",
    "    else:\n",
    "        print('The fit does not have the 1-sigma region between ' + str(alpha_min_allowed) + ' and ' + str(alpha_max_allowed))\n",
    "        good_fit = 1\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.plot(alpha_vector, chi2_vector)\n",
    "        plt.plot(alpha_best, chi2_best, 'd')\n",
    "        plt.plot(alpha_vector, np.ones(n) * (chi2_best + 1), '--r')\n",
    "        plt.xlabel(r'$\\alpha$', fontsize=14)\n",
    "        plt.ylabel(r'$\\chi^2$', fontsize=14)\n",
    "        plt.savefig(savedir + '/fig_bad.png', bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "wtheta_fit_concatenated = wtheta_template(theta_wtheta_data_concatenated, *popt)\n",
    "alpha = popt[0]\n",
    "p0 = popt\n",
    "popt_mean = popt\n",
    "err_params_mean = err_params\n",
    "\n",
    "dof = len(indices_fit)\n",
    "for i in np.arange(0, len(args.bins_removed)):\n",
    "    dof = dof - len(indices_2[args.bins_removed[i]])\n",
    "dof = dof - n_params_eff\n",
    "\n",
    "p_value = chi2.sf(chi_square, dof)\n",
    "\n",
    "alpha_mean_mocks = alpha\n",
    "err_alpha_mean_mocks = err_params[0]\n",
    "\n",
    "results = [round(alpha_mean_mocks, 4),\n",
    "           round(err_alpha_mean_mocks, 4),\n",
    "           round(chi_square, 4),\n",
    "           round(dof, 4)]\n",
    "\n",
    "print(results)\n",
    "\n",
    "np.savetxt(savedir + '/alpha_results_data.txt', results, fmt='%1.4f', newline=' ')\n",
    "np.savetxt(savedir + '/wtheta_data_fit.txt', np.column_stack([err_wtheta_data_concatenated, wtheta_data_concatenated, wtheta_fit_concatenated, err_wtheta_data_concatenated]))\n",
    "np.save(savedir + '/indices_fit.npy', indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d84136-c023-4828-a94d-00175a077ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4632f49-1691-44c1-bb95-ab3fad854fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c11d6-7eb4-4a01-bc4f-9769526cb537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8e5ce-76d8-4774-b78d-ef823b08ca01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb240c-9ee3-4fa1-8e2c-ebf3e4a68516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f646c8-7a00-4dd0-9a29-3112e2ab8345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76673fe-352d-4215-b3cc-9cee04a4f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.9837, 0.0536, 27.135, 33]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306718ac-e93e-4a1a-aaed-f13a9448213b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331ce8d-38a3-4248-a736-cc7eb89f6440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da7856-bcec-4d9d-b24f-fdf7143f058a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37def811-a539-4295-8101-67f0965ea0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c0719-e5ee-4410-b1b3-ced102fbe6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.9449, 0.0331, 74.6871, 84]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi-main",
   "language": "python",
   "name": "cosmodesi-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
