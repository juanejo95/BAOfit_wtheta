{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b5e32-927f-45c1-af60-67a229960586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c500155-c2e9-43d9-98eb-823baea8dd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0961cb7d-2b54-4943-a644-bec2b8f336eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fid n(z), which has 6 redshift bins\n",
      "Saving output to: fit_results/DESY6_1/nzfid_covcosmolike_data_plancktemp_planckcov_deltatheta0.2_thetamin0_thetamax5_3broadband_binsremoved[0, 2, 4, 5]\n",
      "This should be 0: 2.7755575615628914e-17\n",
      "Best-fit alpha = 0.9704 Â± 0.0296\n",
      "chi2/dof = 36.4807/23\n",
      "Best-fit alpha = 0.9709 Â± 0.0412\n",
      "chi2/dof = 36.8172/23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from utils import savedir_baofit, savedir_template, wtheta_model, bao_fit\n",
    "import itertools\n",
    "\n",
    "# # Argument parser setup\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--args.cov_type', help='mocks or cosmolike', default='mocks', type=str)\n",
    "# parser.add_argument('--args.cosmology_template', help='mice, planck or lognormal_Y6BAO', default='mice', type=str)\n",
    "# parser.add_argument('--args.cosmology_covariance', help='mice, planck or lognormal_Y6BAO', default='mice', type=str)\n",
    "# parser.add_argument('--args.diag_only', help='y or n', default='n', type=str)\n",
    "# parser.add_argument('--args.remove_crosscov', help='y or n', default='n', type=str)\n",
    "# parser.add_argument('--args.delta_theta', help='Values: 0.05, 0.1, 0.15 or 0.2', default=0.2, type=float)\n",
    "# parser.add_argument('--args.theta_min', help='min theta to do the BAO fit', default=0.5, type=float)\n",
    "# parser.add_argument('--args.theta_max', help='max theta to do the BAO fit', default=5, type=float)\n",
    "# parser.add_argument('--args.n_broadband', help='1, 2, 3, 4 or 5', default=3, type=int)\n",
    "# parser.add_argument('--args.bins_removed', help='bins removed when doing the BAO fit', default='None', type=str)\n",
    "# parser.add_argument('--args.nz_flag', help='original or modified (shifted+stretched)', default='fid', type=str)\n",
    "# parser.add_argument('--args.include_wiggles', help='y or n', default='y', type=str)\n",
    "# parser.add_argument('--args.weight_type', help='Only allowed if fit_data=1. 0 for no weights, 1 for systematic weights applied', default=1, type=int)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"DESY6\"\n",
    "        self.cov_type = \"cosmolike_data\"\n",
    "        self.cosmology_template = \"planck\"\n",
    "        self.cosmology_covariance = \"planck\"\n",
    "        self.diag_only = \"n\"\n",
    "        self.remove_crosscov = \"n\"\n",
    "        self.delta_theta = 0.2\n",
    "        self.theta_min = 0\n",
    "        self.theta_max = 5\n",
    "        self.n_broadband = 3\n",
    "        self.bins_removed = '0245'\n",
    "        self.nz_flag = \"fid\"\n",
    "        self.include_wiggles = \"y\"\n",
    "        self.weight_type = 1\n",
    "args = Args()\n",
    "args.include_wiggles = '' if args.include_wiggles == 'y' else '_noBAO'\n",
    "\n",
    "# Limits for the fits\n",
    "alpha_min = 0.8\n",
    "alpha_max = 1.2\n",
    "\n",
    "# Handling args.bins_removed argument\n",
    "def generate_bin_mappings():\n",
    "    bin_mappings = {\n",
    "        'None': [],  # Mapping for empty list\n",
    "    }\n",
    "    \n",
    "    # Generate all combinations of numbers from 0 to 6\n",
    "    for r in range(1, 7):  # r is the length of the combination, from 1 to 6\n",
    "        for combo in itertools.combinations(range(7), r):\n",
    "            # Join the numbers in the combination to form a string key\n",
    "            key = ''.join(map(str, combo))\n",
    "            bin_mappings[key] = list(combo)\n",
    "    \n",
    "    return bin_mappings\n",
    "\n",
    "# Generate the mappings\n",
    "bin_mappings = generate_bin_mappings()\n",
    "args.bins_removed = bin_mappings.get(args.bins_removed, args.bins_removed)\n",
    "\n",
    "# Galaxy bias dictionary\n",
    "galaxy_bias = {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}\n",
    "\n",
    "# Redshift distributions\n",
    "from utils import redshift_distributions\n",
    "nz_instance = redshift_distributions(args.nz_flag)\n",
    "\n",
    "# Setting up save directory\n",
    "savedir_instance = savedir_baofit(\n",
    "    include_wiggles=args.include_wiggles, dataset=args.dataset, weight_type=args.weight_type, \n",
    "    nz_flag=args.nz_flag, cov_type=args.cov_type, cosmology_template=args.cosmology_template, \n",
    "    cosmology_covariance=args.cosmology_covariance, delta_theta=args.delta_theta, \n",
    "    theta_min=args.theta_min, theta_max=args.theta_max, n_broadband=args.n_broadband, \n",
    "    bins_removed=args.bins_removed, verbose=True\n",
    ")\n",
    "savedir = savedir_instance()\n",
    "\n",
    "# # Check if the results already exist\n",
    "# if os.path.exists(f\"{savedir}/alpha_results_data.txt\"):\n",
    "#     print(\"This one already done, moving to the next one!\")\n",
    "#     sys.exit()\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Data and theoretical wtheta\n",
    "indices_theta_allbins = {}\n",
    "\n",
    "theta_wtheta_data = {}\n",
    "wtheta_data = {}\n",
    "\n",
    "theta_wtheta_th = {}\n",
    "wtheta_th = {}\n",
    "wtheta_bb_th = {}\n",
    "wtheta_bf_th = {}\n",
    "wtheta_ff_th = {}\n",
    "\n",
    "for bin_z in range(nz_instance.nbins):\n",
    "    if args.dataset == 'DESY6':\n",
    "        filename_wtheta = f'wtheta_data_Yband/wtheta_data_bin{bin_z}_DeltaTheta{args.delta_theta}_weights{args.weight_type}_fstar.txt'\n",
    "    theta, wtheta = np.loadtxt(filename_wtheta).T\n",
    "    \n",
    "    indices_theta_individualbin = np.where((theta > args.theta_min * np.pi / 180) & (theta < args.theta_max * np.pi / 180))[0]\n",
    "\n",
    "    theta_wtheta_data[bin_z] = theta[indices_theta_individualbin]\n",
    "    if bin_z in args.bins_removed:\n",
    "        wtheta_data[bin_z] = np.zeros(len(indices_theta_individualbin))\n",
    "    else:\n",
    "        wtheta_data[bin_z] = wtheta[indices_theta_individualbin]\n",
    "\n",
    "    templates_base = f'wtheta_template{args.include_wiggles}/nz_{args.nz_flag}/wtheta_{args.cosmology_template}'\n",
    "    theta_wtheta_th[bin_z] = np.loadtxt(f'{templates_base}/wtheta_bb_bin{bin_z}.txt')[:, 0]\n",
    "    wtheta_bb_th[bin_z] = np.loadtxt(f'{templates_base}/wtheta_bb_bin{bin_z}.txt')[:, 1]\n",
    "    wtheta_bf_th[bin_z] = np.loadtxt(f'{templates_base}/wtheta_bf_bin{bin_z}.txt')[:, 1]\n",
    "    wtheta_ff_th[bin_z] = np.loadtxt(f'{templates_base}/wtheta_ff_bin{bin_z}.txt')[:, 1]\n",
    "\n",
    "    wtheta_th[bin_z] = (\n",
    "        galaxy_bias[bin_z]**2 * wtheta_bb_th[bin_z] +\n",
    "        galaxy_bias[bin_z] * wtheta_bf_th[bin_z] +\n",
    "        wtheta_ff_th[bin_z]\n",
    "    )\n",
    "\n",
    "    indices_theta_allbins[bin_z] = indices_theta_individualbin + bin_z * len(theta)\n",
    "\n",
    "indices_theta_allbins_concatenated = np.concatenate([indices_theta_allbins[i] for i in range(nz_instance.nbins)])\n",
    "\n",
    "# Interpolate the theoretical wtheta to use it as the template\n",
    "wtheta_th_interp = {}\n",
    "wtheta_bb_th_interp = {}\n",
    "wtheta_bf_th_interp = {}\n",
    "wtheta_ff_th_interp = {}\n",
    "\n",
    "for bin_z in range(nz_instance.nbins):\n",
    "    wtheta_th_interp[bin_z] = interp1d(theta_wtheta_th[bin_z], wtheta_th[bin_z], kind='cubic')\n",
    "    wtheta_bb_th_interp[bin_z] = interp1d(theta_wtheta_th[bin_z], wtheta_bb_th[bin_z], kind='cubic')\n",
    "    wtheta_bf_th_interp[bin_z] = interp1d(theta_wtheta_th[bin_z], wtheta_bf_th[bin_z], kind='cubic')\n",
    "    wtheta_ff_th_interp[bin_z] = interp1d(theta_wtheta_th[bin_z], wtheta_ff_th[bin_z], kind='cubic')\n",
    "\n",
    "# Concatenate the datavector\n",
    "theta_wtheta_data_concatenated = np.concatenate([theta_wtheta_data[i] for i in range(nz_instance.nbins)])\n",
    "\n",
    "wtheta_data_concatenated = np.concatenate([wtheta_data[bin_z] for bin_z in range(nz_instance.nbins)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Covariance matrix\n",
    "if args.cov_type == 'cosmolike_data':\n",
    "    if args.cosmology_covariance == 'mice':\n",
    "        if args.delta_theta != 0.2:\n",
    "            print(f\"No mice cosmolike covariance matrix for delta_theta={args.delta_theta}\")\n",
    "            sys.exit()\n",
    "        cov = np.loadtxt(\n",
    "            f\"cov_cosmolike/cov_Y6bao_data_DeltaTheta{str(args.delta_theta).replace('.', 'p')}_mask_g_mice.txt\"\n",
    "        )\n",
    "    elif args.cosmology_covariance == 'planck':\n",
    "        cov = np.loadtxt(\n",
    "            f\"cov_cosmolike/cov_Y6bao_data_DeltaTheta{str(args.delta_theta).replace('.', 'p')}_mask_g_planck.txt\"\n",
    "        )\n",
    "    theta_cov = np.loadtxt(f\"cov_cosmolike/delta_theta_{args.delta_theta}_binning.txt\")[:, 2] * np.pi / 180\n",
    "\n",
    "# Extend theta covariance across all bins\n",
    "theta_cov_concatenated = np.concatenate([theta_cov] * nz_instance.nbins)\n",
    "\n",
    "# Verify consistency between theta from covariance and data\n",
    "print('This should be 0:', abs(theta_wtheta_data_concatenated - theta_cov_concatenated[indices_theta_allbins_concatenated]).max())\n",
    "if abs(theta_wtheta_data_concatenated - theta_cov_concatenated[indices_theta_allbins_concatenated]).max() > 1e-5:\n",
    "    print('The covariance matrix and the wtheta of the mocks do not have the same theta')\n",
    "    sys.exit()\n",
    "    \n",
    "# Adjust covariance matrix for removed bins\n",
    "cov_adjusted = np.zeros_like(cov)\n",
    "for bin_z1 in range(nz_instance.nbins):\n",
    "    for bin_z2 in range(nz_instance.nbins):\n",
    "        slice_1 = slice(bin_z1 * len(theta_cov), (bin_z1 + 1) * len(theta_cov))\n",
    "        slice_2 = slice(bin_z2 * len(theta_cov), (bin_z2 + 1) * len(theta_cov))\n",
    "        if bin_z1 == bin_z2 or (bin_z1 not in args.bins_removed and bin_z2 not in args.bins_removed):\n",
    "            cov_adjusted[slice_1, slice_2] = cov[slice_1, slice_2]\n",
    "cov = cov_adjusted\n",
    "\n",
    "# Remove cross-covariances\n",
    "if args.remove_crosscov == 'y':\n",
    "    cov_adjusted = np.zeros_like(cov)\n",
    "    for bin_z in range(nz_instance.nbins):\n",
    "        slice_ = slice(bin_z * len(theta_cov), (bin_z + 1) * len(theta_cov))\n",
    "        cov_adjusted[slice_, slice_] = cov[slice_, slice_]\n",
    "    cov = cov_adjusted\n",
    "\n",
    "# Retain only the diagonal\n",
    "if args.diag_only == 'y':\n",
    "    cov = np.diag(np.diag(cov))\n",
    "\n",
    "cov_orig = np.copy(cov)\n",
    "    \n",
    "# Choose the same scale cuts for the covariance matrix that are used for the data\n",
    "cov = cov_orig[indices_theta_allbins_concatenated[:, None], indices_theta_allbins_concatenated]\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "# Create a dictionary with the error in wtheta from the covariance matrix\n",
    "err_wtheta_data = {}\n",
    "\n",
    "for bin_z in range(nz_instance.nbins):\n",
    "    err_wtheta_data[bin_z] = np.sqrt(np.diag(cov_orig))[indices_theta_allbins[bin_z]]\n",
    "err_wtheta_data_concatenated = np.concatenate([err_wtheta_data[i] for i in range(nz_instance.nbins)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Get the template wtheta\n",
    "wtheta_model_instance = wtheta_model(alpha_min, alpha_max, args.n_broadband, theta_wtheta_data, wtheta_th_interp)\n",
    "n_params = len(wtheta_model_instance.names_params)\n",
    "n_params_true = len(wtheta_model_instance.names_params) - (1 + args.n_broadband) * len(args.bins_removed)\n",
    "wtheta_template = wtheta_model_instance.get_wtheta_template()\n",
    "\n",
    "# 4. First fit attempt. We use curve_fit\n",
    "popt, pcov = curve_fit(wtheta_template, theta_wtheta_data_concatenated, wtheta_data_concatenated, p0=wtheta_model_instance.p0, bounds=wtheta_model_instance.bounds, sigma=cov, absolute_sigma=False)\n",
    "err_params = np.sqrt(np.diag(pcov))\n",
    "\n",
    "# Let's compute the chi2 and dof\n",
    "wtheta_fit_concatenated = wtheta_template(theta_wtheta_data_concatenated, *popt)\n",
    "diff = wtheta_data_concatenated - wtheta_fit_concatenated\n",
    "chi_square = diff @ inv_cov @ diff\n",
    "dof = len(wtheta_data_concatenated) - n_params\n",
    "for bin_idx in args.bins_removed:\n",
    "    dof -= len(indices_theta_allbins[bin_idx])\n",
    "\n",
    "print(f'Best-fit alpha = {popt[0]:.4f} Â± {np.sqrt(pcov[0, 0]):.4f}')\n",
    "print(f'chi2/dof = {chi_square:.4f}/{dof}')\n",
    "\n",
    "\n",
    "\n",
    "# 5. With our pipeline\n",
    "bao_fit_instance = bao_fit(wtheta_model_instance, wtheta_data, cov, savedir)\n",
    "alpha_best, chi2_best = bao_fit_instance.fit()\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# np.savetxt(savedir + '/alpha_results_data.txt', results, fmt='%1.4f', newline=' ')\n",
    "# np.savetxt(savedir + '/wtheta_data_fit.txt', np.column_stack([theta_wtheta_data_concatenated, wtheta_data_concatenated, wtheta_fit_concatenated, err_wtheta_data_concatenated]))\n",
    "# # np.save(savedir + '/indices_theta_allbins_concatenated.npy', indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107531d-9fb3-45b9-a5b7-9364db39b1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b64bd9-fc6b-44c4-99d7-de04e58f54c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e5788-ac1a-40b7-89d5-e20a7912f34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi-main",
   "language": "python",
   "name": "cosmodesi-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
